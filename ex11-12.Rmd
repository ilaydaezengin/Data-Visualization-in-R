---
title: "R Notebook"
output: html_notebook
---



```{r}
library(ggplot2)
library(data.table)
library(magrittr)
library(tidyr)
library(ggrepel)
library(caret)
library(plotROC)
library(randomForest)
library(rpart)
```
```{r}
diabetes_dt <- fread("C:/Users/ilayd/OneDrive/Masaüstü/R/extdata_12/extdata/pima-indians-diabetes.csv")
diabetes_dt[, Outcome := as.factor(Outcome)]
# Store feature variables that we will need for later
feature_vars <- colnames(diabetes_dt[,-c("Outcome")])
diabetes_dt[Outcome==1, .N]/nrow(diabetes_dt)
```


```{r}
melted_diabetes_dt <- melt(diabetes_dt[, .(Glucose,
Insulin, Outcome, BloodPressure)],
id.vars="Outcome")
ggplot(melted_diabetes_dt, aes(Outcome, value)) + geom_boxplot() + facet_wrap(~variable, scale = "free")

```


```{r}
logreg_1 <- glm(Outcome ~ Glucose, data = diabetes_dt,family = "binomial")
logreg_1
```


```{r}
summary(logreg_1)
```


```{r}
coeffs <- logreg_1$coefficients
coeffs
```


```{r}
odd_ratios <- exp(coeffs)
odd_ratios
```
```{r}
logreg_2 <- glm(Outcome~BloodPressure,
data = diabetes_dt, family = "binomial")
logreg_3 <- glm(Outcome~Insulin,
data = diabetes_dt, family = "binomial")
logreg_2
summary(logreg_2)
```


```{r}
logreg_3
summary(logreg_3)
```


```{r}
diabetes_dt[, preds_model1 := predict(logreg_1)]
diabetes_dt[, preds_model2 := predict(logreg_2)]
diabetes_dt[, preds_model3 := predict(logreg_3)]
diabetes_dt
```


```{r}
ggplot(melt(diabetes_dt[, .(preds_model1, preds_model2, preds_model3)]), aes(value)) +
geom_histogram() + facet_wrap(~variable, scales="free")
```


```{r}
confusion_matrix <- function(dt, score_column, labels_column, threshold){
# The table() function is very useful for computing the confusion matrix
# We have to use get() to get the column from a string
return(dt[, table(get(labels_column), get(score_column)>threshold) ]) }

thresholds <- c(-1,0,1)
lapply(thresholds, function(t){confusion_matrix(diabetes_dt, "preds_model1", "Outcome", t)})
```


```{r}
confusion_matrix(diabetes_dt, "preds_model1", "Outcome", 1)["0", "TRUE"]
```


```{r}
tpr_fpr <- function(dt, score_column, labels_column, threshold){
# Use confusion matrix
cm <- confusion_matrix(diabetes_dt, score_column, labels_column, threshold)
# determine FP, TP, FN and TN from confusion matrix
TP <- cm["1", "TRUE"]
FP <- cm["0", "TRUE"]
TN <- cm["0", "FALSE"]
FN <- cm["1", "FALSE"]
# compute FPR and TPR
tpr <- TP/(TP+FN)
fpr <- FP/(FP+TN)
return(data.table(tpr=tpr, fpr=fpr, t=threshold))
}
```


```{r}
thresholds <- c(-1,0,1)
dt <- rbindlist(lapply(thresholds, function(t){ tpr_fpr(diabetes_dt, "preds_model1", "Outcome", t) }))
ggplot(dt, aes(fpr, tpr, label=t)) + geom_point() + geom_text_repel()
```
```{r}
plot_dt <- diabetes_dt[,.(Outcome, preds_model1, preds_model2, preds_model3 )] %>%
melt(id.vars="Outcome", variable.name="logistic_fit", value.name="response")
```


```{r}
ggroc <- ggplot(plot_dt, aes(d=as.numeric(Outcome), m=response, color=logistic_fit)) + geom_roc() +
geom_abline() + theme_bw()
# geom_roc does not work with factors so we have to convert Outcome to numeric
aucs <- as.data.table(calc_auc(ggroc))
# Add nice labels including AUC
labels <- sapply(1:3, function(i) {paste("Model", i, ", AUC:", round(aucs[group==i, AUC], 4)) } )
ggroc + scale_color_discrete(name = "Logistic fit", labels = labels)
```


```{r}
full_formula <- as.formula(paste(c("Outcome ~ ", paste(feature_vars, collapse = " + ")),collapse = ""))

logreg_full <- glm(full_formula, data = diabetes_dt, family = "binomial")
logreg_full
```


```{r}
diabetes_dt[, preds_logreg_full := predict(logreg_full)]
ggplot(diabetes_dt, aes(x=preds_logreg_full, fill=Outcome)) +
geom_histogram(position="dodge", bins=50) + theme_bw()
```


```{r}
plot_dt <- diabetes_dt[,.(Outcome, preds_model1, preds_model2, preds_model3, preds_logreg_full )] %>%
melt(id.vars="Outcome", variable.name="logistic_fit", value.name="response")
# Plot roc curves for each model
ggroc <- ggplot(plot_dt, aes(d=as.numeric(Outcome), m=response, color=logistic_fit)) +
geom_roc() +
geom_abline() + theme_bw()
ggroc
```


```{r message=TRUE}
dt_classifier <- rpart(full_formula,
data =diabetes_dt,
control = rpart.control(minsplit = 3, cp = 0.001))
diabetes_dt
```
```{r}
# Save predictions
diabetes_dt[, preds_dt := predict(dt_classifier, type="prob")[,2]]
# Plot roc curve for decision tree
ggroc <- ggplot(diabetes_dt, aes(d=as.numeric(Outcome), m=preds_dt)) +
geom_roc() +
geom_abline() + theme_bw()
ggroc
#result shows overfitting
```


```{r}
## 70% of the data for training
smp_size <- floor(0.70 * nrow(diabetes_dt))
## set the seed to make your partition reproducible
set.seed(13)
train_ind <- sample(seq_len(nrow(diabetes_dt)), size = smp_size)
# label train and test datasets
diabetes_dt[train_ind, dataset:="train"]
diabetes_dt[-train_ind, dataset:="test"]
```


```{r}
## train on training dataset
dt_classifier <- rpart(full_formula,
data =diabetes_dt[train_ind],
control = rpart.control(minsplit = 3, cp = 0))

# get predictions for both train and test set
diabetes_dt[, preds_dt := predict(dt_classifier, type="prob",
# predict on all data, not only on trainset
newdata=diabetes_dt)[,2]]
```


```{r}
ggroc <- ggplot(diabetes_dt, aes(d=as.numeric(Outcome), m=preds_dt, color=dataset)) +
geom_roc() +
geom_abline() + theme_bw()
ggroc
#we see overfitting clearly, model overfits to the training dataset and poorly generalizes the test set.
```


```{r}
rf_classifier <- randomForest(## Define formula and data
full_formula,
## Train only on trainset
data=diabetes_dt[train_ind],
## Hyper parameters
ntree = 200, # Define number of trees
nodesize = 20, # Minimum size of leaf nodes
maxnodes = 7, # Maximum number of leaf nodes
mtry = 5, # Number of feature variables as candidates for each split
sampsize=length(train_ind),
## Output the feature importances
importance=TRUE)
rf_classifier
```
```{r}
diabetes_dt[, preds_rf := predict(rf_classifier, type="prob",
## Predict on all data
newdata=diabetes_dt)[,2]]
# Plot roc curves for each model
ggroc <- ggplot(diabetes_dt, aes(d=as.numeric(Outcome), m=preds_rf, color=dataset)) +
geom_roc() +
geom_abline() + theme_bw()
ggroc
```
```{r}
calc_auc(ggroc)
```


```{r}
diabetes_dt[, Outcome:= ifelse(Outcome==1, "yes", "no")]
# somehow trainControl does not like factors...
# so we convert it to string
```


```{r}
# generate control structure
fitControl <- trainControl(method = "cv",
number = 5, # number of fols
classProbs=TRUE, # display class probabilities
summaryFunction = twoClassSummary)
```


```{r}
logreg_cv <- train(full_formula,
data = diabetes_dt,
## model specification
method = "glm", # we want a logistic regression
family = "binomial",
## validation specification
trControl = fitControl,
## Specify which metric to optimize
metric = "ROC")
logreg_cv
```
```{r}
metrics_dt <- as.data.table(logreg_cv$resample)
metrics_dt[order(-ROC)]
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```
